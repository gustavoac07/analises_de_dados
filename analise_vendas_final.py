# -*- coding: utf-8 -*-
"""analise_vendas_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1unNk-80hRkpX8yp-NXxEpzbkRfBQkBcj

Bibliotecas necessarias para as analises.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Carregando os dados
df_vendas = pd.read_csv('/content/sales.csv')

# Lendo as primeiras linhas
df_vendas.head()

"""Carregando os dados e visualizando as primeiras linhas

* Observamos que as colunas estão em inglês, as colunas (unemployment, fuel_price precisam ser ajustados).
"""

# informações do df

df_vendas.info()

"""Acima buscamos as informações dos dados e os seus tipos.

* Observamos que a coluna Date deve estar em tipo datatime, atualmente está em tipo 'object' (texto).

"""

# Ajustando Fuel_Price e Unemployment (dividir por 1000 para obter os valores corretos)
df_vendas['Fuel_Price'] = df_vendas['Fuel_Price'] / 1000
df_vendas['Unemployment'] = (df_vendas['Unemployment'] / 1000).round(2)

# Convertendo a coluna Date para formato de data
df_vendas['Date'] = pd.to_datetime(df_vendas['Date'], format='%d-%m-%Y')


# Lendo as primeiras linhas após os ajustes
df_vendas.head()

"""Acima realizamos os ajustes necessarios para darmos continuidade a analise de dados.
Alterando:
> Coluna Date de Object para Datetime.

> Realizando as alterações da coluna Fuel_Price e Unemployment dividindo por 1000. Para os ajustes dos valores conforme é necessario para normalização dos dados.
"""

df_vendas.info()

"""Acima novamente observando as informações df e conformando as alterações das colunas mencionadas."""

# Alterando o nome das colunas de inglês para Português

colunas_mapping = {
    'Date': 'Data',
    'Weekly_Sales': 'Vendas_Semanais',
    'Holiday_Flag': 'Flag_Feriado',
    'Temperature': 'Temperatura',
    'Fuel_Price': 'Preco_Combustivel',
    'CPI': 'CPI',
    'Unemployment': 'Desemprego'
}

# Renomeando as colunas dos dados
df_vendas.rename(columns=colunas_mapping, inplace=True)

# Verificando as colunas renomeadas
print(df_vendas.columns)

"""Para melhor identificação das colunas e darmos seguimento a analise de dados, realizamos a atualização das colunas traduzindo-as para o português."""

df_vendas.isnull().sum()

"""Acima para darmos continuidade observamos que não existe colunas nulas no dataset"""

df_vendas.head()

"""Realizando a leitura do dataset confirmando as mudanças.

# **Abaixo para melhor a leitura dos dados e identificar possivéis dados discrepantes como outliers. Vamos utilizar recursos gráficos.**
"""

# plantando os dados em graficos

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Data', y='Vendas_Semanais', data=df_vendas, color='blue', alpha=0.7)
plt.title('Dispersão de Vendas Semanais ao Longo do Tempo')
plt.xlabel('Data')
plt.ylabel('Vendas Semanais')
plt.xticks(rotation=45)
plt.show()

"""Observando a linha do tempo dos dados cada quadrante é 1 trimestre, os circulos em azul representa os valores das vendas da semana.

Observamos poucos valores que a principio parecem outliers porém, eles estão no periodo de feriados como Natal, Ano Novo e Black Friday. Isso justifica vendas acima dos demais.
"""

import seaborn as sns
import matplotlib.pyplot as plt
# Boxplot para 'Preco_Combustivel'
plt.figure(figsize=(12, 6))
sns.boxplot(x='Preco_Combustivel', data=df_vendas, color='lightgreen')
plt.title('Distribuição do Preço do Combustível')
plt.xlabel('Preço do Combustível')
plt.show()

"""Aqui comparando a coluna das Vendas_Semanais com o aumento do preço dos combustiveis, observamos valores outliers uma vez que:

Observamos que os demais valores estão mais dispersos, já dos outliers

*  Uma analise historica economica, em momento algum os combustiveis como esteve nesses valores.
*   Observamos que os demais valores estão mais dispersos, já dos outliers eles apresentam valores continuos.
"""

# valores minimos do preço dos combustiveis

df_vendas ['Preco_Combustivel'].min()

"""Analisando mais detalhadamente, observamos o preço minimo no dataset."""

# Calculando a média dos valores de Preco_Combustivel acima de 0.5
media_precos_validos = df_vendas[df_vendas['Preco_Combustivel'] >= 0.5]['Preco_Combustivel'].mean()

# Substituindo valores menores que 0.5 pela média calculada
df_vendas['Preco_Combustivel'] = df_vendas['Preco_Combustivel'].apply(lambda x: media_precos_validos if x < 0.5 else x)

# Exibindo as primeiras linhas para verificar
df_vendas.head()

"""Assim, resolvemos excluir as linhas relacionadas a essa coluna"""

# plontando valores após o tratamento dos dados menores que 0.5

# Boxplot para 'Preco_Combustivel'
plt.figure(figsize=(12, 6))
sns.boxplot(x='Preco_Combustivel', data=df_vendas, color='lightgreen')
plt.title('Distribuição do Preço do Combustível')
plt.xlabel('Preço do Combustível')
plt.show()

df_vendas ['Preco_Combustivel'].min()

"""Após a reconstrução dos valores, observamos que os dados estão mais coerentes."""

import numpy as np

# Aplicando o logaritmo natural para linearizar
df_vendas['Vendas_Semanais_Log'] = np.log(df_vendas['Vendas_Semanais'])

"""Aplicamos o logaritmo natural em séries temporais e nos dados que exibem uma tendência exponencial, como as da "Vendas_Semanais" é uma técnica estatística importante pela razão:

Linearização de Tendências Exponenciais: Quando os dados crescem ou diminuem exponencialmente, o logaritmo transforma essa curva em uma linha reta, o que facilita a análise e a modelagem. A relação entre as variáveis fica mais simples e pode ser modelada com precisão usando técnicas de regressão, que pressupõem uma relação linear entre as variáveis.
"""

df_vendas.isnull().sum()

"""Novamente antes de darmos continuidade a analise de dados observamos que não existe nenhum dado nulo."""

# consultando quantidade de linhas e colunas após a exclusão

df_vendas.shape

"""Acima, após a mantemos o número de dados e acrescentando a coluna Vendas_Semanais_Log."""

import matplotlib.dates as mdates

# Ajustando o gráfico
plt.figure(figsize=(14, 7))
sns.lineplot(x='Data', y='Desemprego', data=df_vendas, color='blue', linewidth=2)

# Títulos e rótulos
plt.title('Evolução do Desemprego ao Longo do Tempo', fontsize=16, fontweight='bold')
plt.xlabel('Data', fontsize=12)
plt.ylabel('Desemprego', fontsize=12)

# Formatando o eixo x para melhorar a visualização das datas
plt.xticks(rotation=45)
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))

# Adicionando linhas de grade para facilitar a leitura
plt.grid(True, linestyle='--', alpha=0.7)

# Exibindo o gráfico
plt.tight_layout()  # Ajuste automatico do espaçamento
plt.show()

"""Acima a ultima analise de desemprego. Aparentemente não existe interferencias nas compras e desemprego.

Abaixo realizando o calculo e identificando de forma detalhada os valores mais discrepantes e observamos que:
"""

# Definindo o threshold para o Z-score (geralmente 3)
z_threshold = 3

# Calculando o Z-score para cada coluna
df_outliers_zscore = df_vendas[(np.abs((df_vendas[['Vendas_Semanais_Log', 'Temperatura', 'Preco_Combustivel', 'CPI', 'Desemprego']] -
                                df_vendas[['Vendas_Semanais_Log', 'Temperatura', 'Preco_Combustivel', 'CPI', 'Desemprego']].mean()) /
                                df_vendas[['Vendas_Semanais_Log', 'Temperatura', 'Preco_Combustivel', 'CPI', 'Desemprego']].std()) > z_threshold).any(axis=1)]

print("Outliers encontrados (Z-score):")
print(df_outliers_zscore)

# Definindo o fator do IQR (1.5 é comum)
iqr_factor = 1.5

# Calculando o IQR para cada coluna
Q1 = df_vendas[['Vendas_Semanais_Log', 'Temperatura', 'Preco_Combustivel', 'CPI', 'Desemprego']].quantile(0.25)
Q3 = df_vendas[['Vendas_Semanais_Log', 'Temperatura', 'Preco_Combustivel', 'CPI', 'Desemprego']].quantile(0.75)
IQR = Q3 - Q1

# Filtrando os outliers
df_outliers_iqr = df_vendas[((df_vendas[['Vendas_Semanais_Log', 'Temperatura', 'Preco_Combustivel', 'CPI', 'Desemprego']] < (Q1 - iqr_factor * IQR)) |
                      (df_vendas[['Vendas_Semanais_Log', 'Temperatura', 'Preco_Combustivel', 'CPI', 'Desemprego']] > (Q3 + iqr_factor * IQR))).any(axis=1)]

print("Outliers encontrados (IQR):")
print(df_outliers_iqr)

"""Acima os valores outliers estão no dia e semana de feriados importantes e de alto consumo no mercado. Justificando o desempenho maior.
A media de temperatura tambem mais alta por ser verão no país.
Podemos associar que os consumidores podem aumentar as compras de:
* ventilador
* ar-condicionado
* época de recebimento de 13º Salário
* Ferias escolares
* Época de viagens
* Feriados como Natal e Ano Novo
* Consumo alto de bebidas
* Menor desemprego devido aos empregos temporarios

"""

df_vendas.head()

"""# **Analises Preditivas**"""

df_vendas.info()

"""Antes de iniciar as analises preditivas, acima buscamos informações sobre as variaveis. Observamos que a coluna "Data" está em datetime64[ns] que de forma correta por se tratar em de uma coluna data. No entanto, para realizar o modelo preditivo, devemos transformar em float."""

# Extraindo informações de data
df_vendas['Ano'] = df_vendas['Data'].dt.year
df_vendas['Mes'] = df_vendas['Data'].dt.month
df_vendas['Dia'] = df_vendas['Data'].dt.day

"""Dessa forma, acima, dividimos as colunas em: Ano, Mês e Dia. Já que para realizar o testes dos modelos preditivos devemos conter números (float) e não data."""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculando a matriz de correlação
correlacao = df_vendas.corr()

# Exibindo a matriz de correlação com um mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(correlacao, annot=True, cmap='coolwarm', square=True, fmt=".2f")
plt.title('Matriz de Correlação')
plt.show()

"""Observamos que por meio da matriz de correlação, o resultado entre as variaveis dia, mês e ano são baixos.

# **Realizando os testes**
"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Selecionando as variáveis independentes (X) e a variável alvo (y)
X = df_vendas[['Ano','Mes', 'Dia']]
y = df_vendas['Vendas_Semanais_Log']

# Dividindo os dados em conjuntos de treino e teste
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)

# Realizando treino do modelo de regressão linear
modelo = LinearRegression()
modelo.fit(X_treino, y_treino)

# Fazendo as previsões no conjunto de teste
y_prev = modelo.predict(X_teste)

# Avaliando o modelo com MSE e R²
mse = mean_squared_error(y_teste, y_prev)
r2 = r2_score(y_teste, y_prev)

print(f"Erro quadrático médio (MSE): {mse}")
print(f"Coeficiente de determinação (R²): {r2}")

# Comparando os valores reais e previstos
resultados = pd.DataFrame({
    'Vendas_Reais': y_teste,
    'Vendas_Previstas': y_prev
}).reset_index(drop=True)

print(resultados.head())

"""Acima realizamos o primeiro modelo teste com Modelo de Regressão Linear. Utilizamos os testes com as variaveis: dia, mês e ano com a variavel vendas_semanais. Observamos após o treino é que muito baixo: R² de 14%. Ou seja, 14% de confiabilidade. O erro quadrático médio não alto.

Ressaltando que quanto mais variaveis maior é o número de chance de um modelo conseguir um R² maior.
"""

from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Definindo as variáveis X e y usando a nova coluna log-transformada
X = df_vendas[['Ano', 'Dia', 'Mes']]
y = df_vendas['Vendas_Semanais_Log']

# Dividindo os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo Ridge (L2)
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)
y_pred_ridge = ridge_model.predict(X_test)

# Treinando o modelo Lasso (L1)
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_train, y_train)
y_pred_lasso = lasso_model.predict(X_test)

# Avaliando ambos os modelos
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)
mse_lasso = mean_squared_error(y_test, y_pred_lasso)
r2_lasso = r2_score(y_test, y_pred_lasso)

print(f"Ridge MSE: {mse_ridge}, R²: {r2_ridge}")
print(f"Lasso MSE: {mse_lasso}, R²: {r2_lasso}")

"""Neste código, foram utilizados dois modelos de regressão regularizada: Ridge (L2) e Lasso (L1). O objetivo é prever a variável transformada das "Vendas Semanais" (Vendas_Semanais_Log), usando algumas variáveis independentes como ano, mês e dia.

Preparação dos Dados:

Definiram-se as variáveis independentes X (ano, mês e dia) e a variável dependente y (log das vendas semanais).
Em seguida, os dados foram divididos em dois conjuntos: treino (80%) e teste (20%) usando train_test_split. Isso permite avaliar o modelo com dados que não foram usados durante o treinamento.

Treinamento do Modelo Ridge (L2):

O modelo Ridge foi treinado com um parâmetro de regularização alpha=1.0. Esse modelo ajuda a diminuir coeficientes nas variáveis independentes, o que ajuda a evitar overfitting (sobreajuste).

Treinamento do Modelo Lasso (L1):

O modelo Lasso foi treinado com um parâmetro alpha=0.1, que também aplica ajuste, mas com foco em zerar alguns coeficientes. Isso é útil quando se deseja realizar seleção de variáveis, ou seja, eliminar variáveis que não têm influência significativa no modelo.

Avaliação dos Modelos:

Para ambos os modelos, foram calculadas duas métricas de desempenho:
MSE (Mean Squared Error): Mede o erro médio quadrático entre as previsões e os valores reais. Quanto menor, melhor.
R² (Coeficiente de Determinação): Mede a proporção da variabilidade da variável dependente que é explicada pelo modelo. Quanto mais próximo de 1, melhor.

**Interpretando**

Interpretação dos Resultados:

Ridge obteve um desempenho melhor que Lasso, com um R² positivo (0.14), o que significa que, embora o modelo não seja preciso, ele é mais capaz de capturar a relação entre as variáveis em comparação com o Lasso.
O Lasso teve um R² negativo, indicando que o modelo não conseguiu representar bem os dados.
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Aplicando uma transformação polinomial de grau 2
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Dividindo em treino e teste
X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Treinando o modelo de Regressão Linear Polinomial
poly_model = LinearRegression()
poly_model.fit(X_train_poly, y_train_poly)
y_pred_poly = poly_model.predict(X_test_poly)

# Avaliando o modelo polinomial
mse_poly = mean_squared_error(y_test_poly, y_pred_poly)
r2_poly = r2_score(y_test_poly, y_pred_poly)

print(f"Polinomial MSE: {mse_poly}, R²: {r2_poly}")

"""Para gerar novas características a partir das variáveis originais. A transformação polinomial de grau 2 cria novas variáveis (termos quadráticos) e combinações das variáveis existentes. No caso de um grau 2, para cada variável original.
O objetivo da transformação polinomial é modelar relações não-lineares entre as variáveis independentes e a variável dependente (no seu caso, Vendas_Semanais_Log), permitindo que o modelo de regressão linear aprenda essas relações complexas.

Divisão dos Dados:

Após transformar as variáveis, dividimos os dados em conjuntos de treino (80%) e teste (20%) com train_test_split. Essa divisão é fundamental para avaliar o modelo com dados que não foram usados durante o treinamento, ajudando a evitar o sobreajuste (overfitting).
Treinamento do Modelo de Regressão Linear:

Apesar do modelo seja linear, ele agora trabalha com as variáveis transformadas polinomialmente, o que permite modelar relações não-lineares.

**Nova tentativa com modelo para não lineares**

A intenção é testar os modelos conforme os dados disponibilizados e o o Gradient Boosting geralmente oferece um desempenho muito bom, superando modelos de regressão simples.

* O modelo pode ser robusto a outliers quando bem ajustado, pois o boosting foca em corrigir os erros, não apenas nas grandes variações.
* Pode capturar tanto relações lineares quanto não-lineares de maneira eficiente.

Como a base de dados apresenta não-lineares, mesmo com os log natural realizamos o teste para teste de comportamento e como seria os resultados de R² e MSE.
"""

from sklearn.ensemble import GradientBoostingRegressor

# Treinando o modelo
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr_model.fit(X_train, y_train)

# Fazendo previsões
y_pred_gbr = gbr_model.predict(X_test)

# Avaliando o modelo
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"Gradient Boosting - MSE: {mse_gbr}")
print(f"Gradient Boosting - R²: {r2_gbr}")

"""Apesar de um R² alto, o MSE configura anomalia no modelo testado. Aparentemente o modelo caiu em Overfitting."""

# Calcular MSE no treinamento
y_train_pred = gbr_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)

# Calcular MSE no teste
mse_test = mean_squared_error(y_test, y_pred_gbr)

print(f"MSE no treinamento: {mse_train}")
print(f"MSE no teste: {mse_test}")

"""Com base nos resultados apresentados, é possível afirmar que o modelo está passando por overfitting, ou seja, ele aprendeu excessivamente sobre os dados de treino e tem dificuldades em generalizar para dados novos. A baixa performance no teste, em comparação com o treinamento, indica que o modelo não consegue capturar padrões de maneira eficiente quando exposto a dados não vistos. O overfitting compromete a capacidade do modelo de fornecer previsões precisas em cenários do mundo real, onde os dados podem variar.

# **Conclusão**

Os valores disponibilidades pela Melhores Compras não são suficientes para realizar o modelo preditivo de forma satisfatoria. Motivos:


1.   Base de Dados pequeno impossibilitando melhor treino e teste dos dados para o modelo preditivo.

2.   Os dados são insuficientes uma vez que existem variaveis que não é possivel realizar de forma assertiva o modelo e treino.

3.   O desafio é entregar o melhor modelo para prever as vendas para ponto futuros, porém a base de dados entregue tem vendas totais por semana, demais variaveis não contribuem de forma direta para ajustes necessarios. Por exemplo: temperatura tem média para cada semana.


O que confirma a hipotese é que mesmo após o tratamento das variaveis:
preços de combustiveis e desemprego por 1000, identificação de outliers no:

*   Preços de combustiveis e desemprego por 1000
*   Identificação de outliers no preço de combustiveis e realizando os seus tratamento
*   Normalização da variavel data para datatimes.
*   Transformação da variavel vendas_semanais por log natural na tentativa de realizar um linearização do dado para melhor performace da variavel para o modelo.
*   Transformando data em foat para realizar o modelo preditivo
*   Utilizando diversas formas das variaveis relacionadas ao desafio como dia, mês e ano com vendas_semanais e vendas_semanais_log.


Observamos que:


1. Realizamos o modelo de regressão linear, o R² muito baixo e MSE muito alto o que significa com os valores não estão adequados.

2. Realizamos modelos com diversas variaveis diferente e novamente os valores do R² houve melhora no R² e MSE porém após o teste de overfitting, observamos que não podemos nos basear no teste Gradient Boosting.

3. Utilizamos para tentar melhorar o desempenho e transformou em polinomial de grau 2 cria novas variáveis (termos quadráticos) e combinações das variáveis existentes. O objetivo foi transformar polinomial e modelar relações não-lineares entre as variáveis independentes e a variável dependente.

4. Apesar de uma melhora no R² e MSE baixo, ainda apresenta um retumbante nivel baixo de confiabilidade de 30%.

Dessa forma apresentaremos de forma DataViz para reforçar os insights extraidos.

# **Data Viz**
"""

import matplotlib.pyplot as plt

# Configurando o tamanho da figura para melhor visualização
plt.figure(figsize=(14, 6))

# Gráfico da variável original
plt.subplot(1, 2, 1)
plt.plot(df_vendas['Data'], df_vendas['Vendas_Semanais'], color='blue')
plt.title('Vendas Semanais (Original)')
plt.xlabel('Data')
plt.ylabel('Vendas Semanais')
plt.grid(True)

# Gráfico da variável log-transformada
plt.subplot(1, 2, 2)
plt.plot(df_vendas['Data'], df_vendas['Vendas_Semanais_Log'], color='green')
plt.title('Vendas Semanais (Log-transformada)')
plt.xlabel('Data')
plt.ylabel('Log(Vendas Semanais)')
plt.grid(True)

# Exibindo os gráficos lado a lado
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Plotando para ver o resultado da transformação
plt.figure(figsize=(10, 5))

# Gráfico transformado
plt.plot(df_vendas['Data'], df_vendas['Vendas_Semanais_Log'], color='green')
plt.title('Vendas Semanais (Log-transformada)')
plt.xlabel('Data')
plt.ylabel('Log(Vendas Semanais)')
plt.grid(True)
plt.show()

"""Acima vemos que existe:

* Variação significativa: As vendas semanais apresentam uma variação considerável ao longo do tempo, com picos e vales pronunciados.

* Sazonalidade: Há indícios de um padrão sazonal nas vendas, com picos e vales que se repetem em períodos semelhantes do ano.

* Tendência de crescimento: Ao longo do período analisado, observa-se uma tendência geral de crescimento nas vendas.

Gráfico com Transformação Logarítmica:

* Estabilização da variância: A aplicação da transformação logarítmica ajudou a estabilizar a variância dos dados, tornando a série temporal mais homogênea.
* Visualização de detalhes: A transformação logarítmica pode ajudar a visualizar detalhes mais finos na série temporal, como pequenas flutuações que poderiam estar sendo "afogadas" pela alta variabilidade dos dados originais.
* Manutenção da sazonalidade e tendência: A transformação logarítmica não alterou a natureza sazonal e a tendência de crescimento observadas nos dados originais.
"""

# baixar o df_vendas atualizado em excel.

# df_vendas.to_excel('df_vendas.xlsx', index=False)

import matplotlib.pyplot as plt
import seaborn as sns

# Comparação dos dados reais vs previstos (regressão polinomial)
plt.figure(figsize=(10, 6))

# Dados reais e previstos
plt.scatter(y_test_poly, y_pred_poly, color='blue', label='Previsões vs Reais')

# Linha de identidade
plt.plot([min(y_test_poly), max(y_test_poly)], [min(y_test_poly), max(y_test_poly)], color='red', linestyle='--', label='Linha de Identidade')

# Rótulos
plt.title('Comparação entre Dados Reais e Previstos (Regressão Polinomial)')
plt.xlabel('Valores Reais')
plt.ylabel('Valores Previstos')
plt.legend()

# Exibindo o gráfico
plt.show()

"""Este gráfico é uma ferramenta visual crucial para avaliar o desempenho de um modelo de regressão polinomial. Ele compara os valores reais de uma variável (eixo x) com os valores previstos pelo modelo (eixo y).

Pontos azuis: Cada ponto representa um par de valores: o valor real de uma observação e o valor previsto pelo modelo para essa mesma observação.
Linha vermelha tracejada: Essa linha representa a linha de identidade, onde os valores previstos são exatamente iguais aos valores reais. Se os pontos estiverem próximos dessa linha, significa que o modelo está fazendo previsões precisas.
"""

# Gráfico de Resíduos (erro) entre os valores reais e previstos
plt.figure(figsize=(10, 6))

# Calculando os resíduos
residuals = y_test_poly - y_pred_poly

# Plotando os resíduos
sns.scatterplot(x=y_pred_poly, y=residuals, color='green')

# Adicionando a linha de zero (sem erro)
plt.axhline(y=0, color='red', linestyle='--')

# Rótulos
plt.title('Gráfico de Resíduos')
plt.xlabel('Valores Previstos')
plt.ylabel('Resíduos')

# Exibindo o gráfico
plt.show()

"""O gráfico de resíduos é uma ferramenta fundamental para avaliar a qualidade de um modelo de regressão. Ele nos ajuda a identificar padrões e desvios nos dados que podem indicar problemas com o modelo ou com as premissas da regressão.

O que o gráfico mostra:

Eixo X: Valores previstos pelo modelo.
Eixo Y: Resíduos, que representam a diferença entre os valores reais e os valores previstos pelo modelo (Resíduo = Valor Real - Valor Previsto).
Linha horizontal vermelha: Representa a linha de média zero, ou seja, o ponto onde os resíduos se concentrariam se o modelo fosse perfeito e os erros fossem aleatórios.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculando a matriz de correlação
correlacao = df_vendas.corr()

# Exibindo a matriz de correlação com um mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(correlacao, annot=True, cmap='coolwarm', square=True, fmt=".2f")
plt.title('Matriz de Correlação')
plt.show()

"""Com base na sua matriz de correlação, podemos destacar algumas relações importantes:

* Vendas Semanais e Vendas Semanais Log: Existe uma correlação positiva muito forte entre as vendas semanais originais e as vendas semanais transformadas em logaritmo. Isso é esperado, pois a transformação logarítmica geralmente preserva a ordem das observações.
Vendas Semanais e Ano: Há uma correlação positiva moderada entre as vendas semanais e o ano. Isso sugere que as vendas tendem a aumentar ao longo do tempo.

* Vendas Semanais e Preço do Combustível: Existe uma correlação positiva moderada entre as vendas semanais e o preço do combustível. Isso pode indicar que um aumento no preço do combustível está associado a um aumento nas vendas, o que poderia ser investigado mais a fundo para entender a razão dessa relação. Pois, na verdade, pode estar relacionado a inflação e não necessariamente ao aumento de vendas.

* Vendas Semanais e Desemprego: Há uma correlação negativa moderada entre as vendas semanais e o desemprego. Isso sugere que quando o desemprego aumenta, as vendas tendem a diminuir, o que é intuitivo, pois com mais pessoas desempregadas, o consumo tende a cair.
Além de estar associado a momento macroeconomico, politicos e momentos especificos como final de ano onde existe maior empregabilidade pelos trabalhos temporarios.

* CPI e Desemprego: Existe uma forte correlação negativa entre o índice de preços ao consumidor (CPI) e o desemprego. Isso indica que quando o CPI aumenta (ou seja, há inflação), o desemprego tende a aumentar também, o que é uma relação econômica comum.
"""

